# Term Glossary

This term glossary aims to provide short information on common abbreviations used in the space of AI & LLMs and then provide a link that dives deeper into the topic if you are interested.

| Abbreviation | Expansion                                               | Definition                                                   | Link                                                         |
| ------------ | ------------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| AoE          | Assembly-of-Experts                                     | A conceptual or architectural approach where multiple specialized "experts" (e.g., smaller models or modules) are combined to address different parts of a complex problem, with a mechanism to assemble their individual contributions into a coherent output. | https://arxiv.org/pdf/2506.14794                             |
| MoE          | Mixture-of-Experts                                      | A neural network architecture where different "expert" sub-networks specialize in processing different types of inputs, and a "router" or "gate" network learns to activate the most suitable experts for a given input, often leading to more efficient training and inference for large models. | https://huggingface.co/blog/moe                              |
| LLM          | Large Language Model                                    | A type of artificial intelligence model, typically based on transformer architecture, trained on vast amounts of text data to understand, generate, and process human language for various tasks like translation, summarization, and conversation. | https://www.youtube.com/watch?v=LPZh9BOjkQs                  |
| RAG          | Retrieval Augmented Generation                          | An AI framework that combines a retrieval component with a generative model (like an LLM). When a query is given, relevant information is first retrieved from a knowledge base, and then the generative model uses this retrieved information to formulate a more informed and accurate response. | https://arxiv.org/pdf/2312.10997                             |
| API          | Application Programming Interface                       | A set of rules, protocols, and tools that allows different software applications to communicate and interact with each other. It defines the methods and data formats that applications can use to request and exchange information. | https://aws.amazon.com/what-is/api/                          |
| SDK          | Software Development Kit                                | A collection of software development tools in one installable package. SDKs typically include libraries, debuggers, sample code, documentation, and other resources that help developers create applications for a specific platform or system. | https://nordicapis.com/what-is-the-difference-between-an-api-and-an-sdk/ |
| MoGE         | Mixture-of-Grouped-Experts                              | A variation or extension of the Mixture-of-Experts (MoE) architecture where experts are organized into groups, and the routing mechanism might select a group of experts rather than individual ones, potentially offering different trade-offs in terms of specialization and computational cost. | https://arxiv.org/pdf/2505.21411                             |
| NPU          | Neural Processing Unit                                  | A specialized hardware accelerator designed to efficiently run artificial neural networks and machine learning workloads. NPUs are optimized for parallel processing of tensor operations, which are common in AI computations. | https://www.ibm.com/think/topics/neural-processing-unit      |
| GPU          | Graphics Processing Unit                                | A specialized hardware accelerator designed to rapidly manipulate and alter memory to accelerate the creation of images, frames, and video for output to a display device. GPUs are highly parallel and are widely used for general-purpose computing, especially in AI and machine learning. | https://www.ibm.com/think/topics/gpu                         |
| CPU          | Central Processing Unit                                 | The primary component of a computer that performs most of the processing inside a computer, carrying out the instructions of a computer program by performing the basic arithmetic, logic, controlling, and input/output (I/O) operations. | https://www.ibm.com/think/topics/central-processing-unit     |
| TPU          | Tensor Processing Unit                                  | An application-specific integrated circuit (ASIC) developed by Google specifically for accelerating machine learning workloads. TPUs are optimized for the mathematical operations common in neural networks, particularly tensor computations. | https://cloud.google.com/tpu?hl=en                           |
| AI           | Artificial Intelligence                                 | The simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and language understanding. | https://www.sas.com/en_gb/insights/analytics/what-is-artificial-intelligence.html |
| AGI          | Artificial General Intelligence                         | A hypothetical type of AI that possesses the ability to understand, learn, and apply intelligence to any intellectual task that a human being can. It implies broad cognitive capabilities, not just specialized expertise. | https://aws.amazon.com/what-is/artificial-general-intelligence/ |
| ASI          | Artificial Super Intelligence                           | A hypothetical intelligence that would significantly surpass the cognitive abilities of the smartest and most gifted human minds in virtually every field, including scientific creativity, general wisdom, and social skills. | https://www.ibm.com/think/topics/artificial-superintelligence |
| GPT          | Generative Pre-trained Transformer                      | A family of large language models developed by OpenAI. They are "generative" because they can produce new text, "pre-trained" on massive datasets, and based on the "Transformer" neural network architecture. | https://www.ibm.com/think/topics/gpt                         |
| SOTA         | State-of-the-Art                                        | Refers to the most advanced or highest level of development, methods, or techniques achieved in a particular field at a particular time. In AI, it often describes models or algorithms that currently yield the best performance on specific benchmarks. | https://www.e2enetworks.com/blog/what-is-sota-in-artificial-intelligence |
| SLM          | Small Language Model                                    | A language model that is significantly smaller in terms of parameters and computational requirements compared to LLMs. SLMs are often designed for efficiency, edge deployment, or specialized tasks where the full power of an LLM isn't necessary. | https://www.ibm.com/think/topics/small-language-models       |
| BERT         | Bidirectional Encoder Representations from Transformers | A transformer-based machine learning model developed by Google for natural language processing pre-training. Unlike traditional unidirectional models, BERT processes text bidirectionally, considering the context from both left and right sides of a word. | https://arxiv.org/pdf/1810.04805                             |
| RL           | Reinforcement Learning                                  | A type of machine learning where an agent learns to make decisions by performing actions in an environment to maximize a cumulative reward. The agent learns through trial and error, receiving feedback in the form of rewards or penalties. | https://aws.amazon.com/what-is/reinforcement-learning/       |
| RLHF         | Reinforcement Learning from Human Feedback              | A technique used to align large language models with human preferences and values. It involves training a reward model based on human judgments of model outputs, and then using this reward model to fine-tune the LLM with reinforcement learning. | https://huggingface.co/blog/rlhf                             |
| NN           | Neural Network                                          | A computing system inspired by the structure and function of biological neural networks in the human brain. It consists of interconnected nodes (neurons) organized in layers, which process and learn from data. | https://www.sas.com/en_us/insights/analytics/neural-networks.html |
| DL           | Deep Learning                                           | A subfield of machine learning that uses neural networks with many layers (deep neural networks) to learn complex patterns from large amounts of data. It has been particularly successful in areas like image recognition, speech recognition, and natural language processing. | https://arxiv.org/pdf/1404.7828                              |
| CNN          | Convolutional Neural Network                            | A class of deep neural networks primarily used for analyzing visual imagery. CNNs use specialized layers called convolutional layers to automatically and adaptively learn spatial hierarchies of features from input data. | https://arxiv.org/pdf/1511.08458                             |
| RNN          | Recurrent Neural Network                                | A type of neural network designed to process sequential data (like time series, natural language, or video) by maintaining an internal "memory" or hidden state that captures information from previous steps in the sequence. This allows RNNs to understand context and dependencies across different points in the data. | https://aws.amazon.com/what-is/recurrent-neural-network/     |
| LSTM         | Long Short-Term Memory                                  | A type of recurrent neural network (RNN) architecture designed to better handle and learn long-term dependencies in sequential data. LSTMs have internal memory cells that allow them to selectively remember or forget information over extended periods. | https://arxiv.org/pdf/1909.09586                             |
| SSM          | State-Space-Models                                      | A class of mathematical models that represent a system's behavior using a set of internal "state" variables. In the context of deep learning, certain architectures (like Mamba) are inspired by classical state-space models to efficiently process sequential data. | https://huggingface.co/blog/lbourdois/get-on-the-ssm-train   |
| MCP          | Model Context Protocol                                  | The Model Context Protocol is an open standard, open-source framework introduced by Anthropic in November 2024 to standardize the way artificial intelligence systems like large language models integrate and share data with external tools, systems, and data sources. | https://modelcontextprotocol.io/introduction                 |
| KDE          | Kernel Density Estimation                               | A non-parametric way to estimate the probability density function of a random variable. KDE is used to smooth out data points and create a continuous probability distribution, often used for data visualization or outlier detection. | https://towardsdatascience.com/kernel-density-estimation-explained-step-by-step-7cc5b5bc4517/ |
| CoT          | Chain-of-Thought                                        | A prompting technique used with large language models to encourage them to break down a complex problem into intermediate steps and show their reasoning process explicitly. This often leads to more accurate and reliable answers. | https://arxiv.org/pdf/2201.11903                             |
| SAM          | Segment Anything Model                                  | A promptable image segmentation model developed by Meta AI. It can generate high-quality object masks for any object in an image or video, given various input prompts like points, boxes, or text. | https://arxiv.org/abs/2304.02643                             |
| LoRA         | Low-Rank Adaptation                                     | A parameter-efficient fine-tuning technique for large pre-trained models. Instead of fine-tuning all the model's parameters, LoRA injects small, low-rank matrices into the model's layers, significantly reducing the number of trainable parameters and computational cost. | https://arxiv.org/pdf/2106.09685                             |
| QLoRA        | Quantized Low-Rank Adaptation                           | An extension of LoRA that further reduces memory usage by quantizing the pre-trained model to a lower precision (e.g., 4-bit) and then applying LoRA adapters. This allows for fine-tuning very large models on consumer-grade GPUs. | https://openreview.net/pdf?id=aJnKjvTtPq                     |
| TTS          | Text-to-Speech                                          | A technology that converts written text into spoken audio. It enables computers to read digital text aloud, making information accessible to a wider audience and powering voice assistants and other applications. | https://arxiv.org/pdf/2401.13891                             |
| STT          | Speech-to-Text                                          | A technology that converts spoken language into written text. Also known as speech recognition, it is used in voice assistants, transcription services, and many accessibility features. | https://www.ibm.com/think/topics/speech-to-text              |
| VLM          | Vision Language Model                                   | A type of AI model that integrates both visual and linguistic understanding. VLMs can process and reason about information from both images (or video) and text, enabling tasks like image captioning, visual question answering, and multimodal generation. | https://huggingface.co/blog/vlms                             |
| TTFT         | Time to First Token                                     | A metric used in generative AI, particularly with language models, that measures the time taken from when a request is made until the very first output token (word or sub-word) is generated by the model. It's an indicator of initial responsiveness. | https://docs.nvidia.com/nim/benchmarking/llm/latest/metrics.html |
| TPOT         | Time per Output Token                                   | A metric used in generative AI that measures the average time taken to generate each subsequent output token after the first one. It indicates the sustained generation speed of a model. | https://learn.microsoft.com/en-us/azure/databricks/machine-learning/foundation-model-apis/prov-throughput-run-benchmark |
| TPS          | Tokens per Second                                       | A metric used to quantify the throughput or speed of a generative AI model, particularly in language generation. It represents the number of tokens (words or sub-words) that the model can generate per second. | https://docs.nvidia.com/nim/benchmarking/llm/latest/metrics.html |
| KV cache     | Key-Value cache                                         | In transformer models, the KV cache is a mechanism used during inference to store the computed "key" and "value" vectors for previously processed tokens. This prevents redundant re-computation of these attention components for each new token, significantly speeding up autoregressive generation. | https://huggingface.co/blog/not-lain/kv-caching              |
| CUDA         | Compute Unified Device Architecture                     | A parallel computing platform and API model developed by NVIDIA for its GPUs. CUDA allows software developers to use a CUDA-enabled GPU for general-purpose processing, greatly accelerating computationally intensive tasks, especially in AI and scientific computing. | https://developer.nvidia.com/blog/even-easier-introduction-cuda/ |
| MoM          | Mixture of Models                                       | A general concept in machine learning where multiple individual models (often of different types or trained on different aspects of data) are combined, and their outputs are somehow weighted or aggregated to produce a final result. This can improve robustness, accuracy, or handle diverse data characteristics compared to a single model. It is a broader concept that can encompass specific architectures like Mixture-of-Experts (MoE). It has gained traction to send the same request to multiple LLMs. | https://promptmetheus.com/resources/llm-knowledge-base/mixture-of-models-mom |
| GAN          | Generative Adversarial Networks                         | A type of artificial intelligence framework composed of two neural networks: a generator and a discriminator. The generator creates new data samples (e.g., images, text, audio) that resemble real data, while the discriminator tries to distinguish between the real data and the fake data produced by the generator. The two networks are trained in an adversarial (competing) process, where the generator tries to fool the discriminator, and the discriminator tries to accurately identify the fakes, leading to the generation of increasingly realistic synthetic data. | https://arxiv.org/pdf/1406.2661                              |
| SAE          | Sparse Autoencoder                                      | A type of neural network autoencoder that is trained to learn a sparse representation (encoding) of its input data. This means that in the hidden layer(s), only a small number of neurons are "active" (non-zero) for any given input, while the rest are zero or close to zero. SAEs are often used for dimensionality reduction, feature extraction, and, increasingly, for interpreting the internal representations of large neural networks by identifying "interpretable features". | https://adamkarvonen.github.io/machine_learning/2024/06/11/sae-intuitions.html |
| USAE         | Universal Sparse Autoencoder                            | A type of Sparse Autoencoder designed to learn generalizable and interpretable features across a wide range of tasks and models, particularly within large language models. The "universal" aspect implies that the learned features are not specific to a single model or dataset but can represent meaningful concepts that emerge consistently across different parts of a neural network's internal representations. | https://arxiv.org/pdf/2502.03714                             |
| CLIP         | Contrastive Language–Image Pre-training                 | A neural network trained by OpenAI to learn visual concepts from natural language supervision. It works by training an image encoder and a text encoder simultaneously to predict which out of a batch of 32,768 randomly sampled text snippets was actually paired with a given image. This allows CLIP to understand the semantic relationship between images and text, enabling zero-shot recognition of objects and concepts in images without explicit training on those specific categories. | https://openai.com/index/clip/                               |
| SFT          | Supervised Fine-Tuning                                  | A method of adapting a pre-trained large language model (LLM) to a specific task or dataset by continuing its training on a smaller, task-specific dataset with labeled examples. In SFT, the model learns to generate the desired outputs for given inputs, guided by explicit correct answers, often using techniques like next-token prediction or sequence-to-sequence learning. | https://arxiv.org/html/2412.13337v1                          |
| MAS          | Multi-Agent System                                      | A system composed of multiple interacting intelligent agents that cooperate, coordinate, or negotiate to achieve common goals or solve complex problems that are difficult or impossible for a single agent to handle alone. Each agent typically has its own capabilities, knowledge, and goals, and interacts with others to achieve the system's objectives. | https://www.ibm.com/think/topics/multiagent-system           |

